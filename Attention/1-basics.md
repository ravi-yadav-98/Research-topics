## Basics of attention mechanism:
### Introduction:
  - Self-attention was proposed by researchers at Google Research and Google Brain. 
  - It was proposed due to challenges faced by encoder-decoder in dealing with long Sequence
  - ![image](https://user-images.githubusercontent.com/85448160/125396411-234bc800-e3ca-11eb-9892-9154c8b66d88.png)

  - The attention mechanism allows output to focus attention on input while producing output while the self-attention model allows inputs to interact with each other
  - Transformers use attention mechanism.
  - 
